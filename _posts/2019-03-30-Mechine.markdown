---

layout: post

title: "[머신러닝] 머신러닝 기초 정리 (역전파)"

---


### 역전파

심층 신경망을 학습하는데 유용하게 활용되는 역전파(backpropagtion)을 알아보겠습니다.


###계산그래프와 chain rule
계산그래프는 계산과정을 그래프로 나타낸 것입니다. 노드는 함수, 엣지는 값을 뜻한다고 하였을때, y=f(x)를 나타내는 계산 그래프는 아래그림과 같습니다.
![Alt text](/images/2019-03-30-Mechine/ML_1.png)
계산그래프에서 계산을 왼쪽에서 오른쪽으로 진행하는 단계를 순전파(forward propagation)라고 합니다. 위 그림 기준으로는 녹색 화살표가 됩니다. 입력값 x는 함수 f를 거쳐 y로 순전파되고 있는 점을 확인할 수 있습니다.

반대로 계산을 오른쪽에서 왼쪽으로 진행하는 단계를 역전파(backward propagation)라고 합니다. 빨간색 화살표가 역전파를 가리킵니다./

여기에서 ∂L/∂y의 의미에 주목할 필요가 있습니다. 지금은 예시이기 때문에 노드를 하나만 그렸지만, 실제 뉴럴네트워크는 이러한 노드가 꽤 많은 큰 계산그래프입니다. 이 네트워크는 최종적으로는 정답과 비교한 뒤 Loss를 구합니다.

우리의 목적은 뉴럴네트워크의 오차를 줄이는 데 있기 때문에, 각 파라메터별로 Loss에 대한 그래디언트를 구한 뒤 그래디언트들이 향한 쪽으로 파라메터들을 업데이트합니다. ∂L/∂y는 y에 대한 Loss의 변화량, 즉 Loss로부터 흘러들어온 그래디언트라고 이해하면 좋을 것 같습니다.

이제는 현재 입력값 x에 대한 Loss의 변화량, 즉 ∂L/∂x를 구할 차례입니다. 이는 미분의 연쇄법칙(chain rule)에 의해 다음과 같이 계산할 수 있습니다.

이미 설명드렸듯 ∂L/∂y는 Loss로부터 흘러들어온 그래디언트입니다. ∂y/∂x는 현재 입력값에 대한 현재 연산결과의 변화량, 즉 로컬 그래디언트(Local Gradient)입니다.

다시 말해 현재 입력값에 대한 Loss의 변화량은 Loss로부터 흘러들어온 그래디언트에 로컬 그래디언트를 곱해서 구한다는 이야기입니다. 이 그래디언트는 다시 앞쪽에 배치돼 있는 노드로 역전파됩니다.

### 덧셈 노드
덧셈 노드의 수식은 아래와 같습니다.

z=f(x,y)=x+y
덧셈 노드의 로컬 그래디언트는 아래와 같습니다.
![Alt text](/images/2019-03-30-Mechine/ML_2.png)

∂z∂x=∂(x+y)∂x=1∂z∂y=∂(x+y)∂y=1
덧셈 노드의 계산그래프는 아래와 같습니다. 현재 입력값에 대한 Loss의 변화량은 로컬 그래디언트에 흘러들어온 그래디언트를 각각 곱해주면 됩니다. 덧셈 노드의 역전파는 흘러들어온 그래디언트를 그대로 흘려보내는 걸 확인할 수 있습니다.

### 곱셈 노드
곱셈 노드의 수식은 아래와 같습니다.

z=f(x,y)=xy
곱셈 노드의 로컬 그래디언트는 아래와 같습니다.
![Alt text](/images/2019-03-30-Mechine/ML_3.png)

∂z∂x=∂(xy)∂x=y∂z∂y=∂(xy)∂y=x
곱셈 노드의 계산그래프는 아래와 같습니다. 현재 입력값에 대한 Loss의 변화량은 로컬 그래디언트에 흘러들어온 그래디언트를 각각 곱해주면 됩니다. 곱셈 노드의 역전파는 순전파 때 입력 신호들을 서로 바꾼 값을 곱해서 하류로 흘려보내는 걸 확인할 수 있습니다.

### ReLU 노드
활성화함수(activation function)로 사용되는 ReLU는 다음 식처럼 정의됩니다.

y=x(x>0)y=0(x≤0)
ReLU 노드의 로컬 그래디언트는 아래와 같습니다.

∂y∂x=1(x>0)∂y∂x=0(x≤0)
계산그래프는 아래와 같습니다.
![Alt text](/images/2019-03-30-Mechine/ML_4.png)
### Sigmoid 노드
시그모이드(sigmoid) 함수는 아래와 같이 정의됩니다.

y=11+exp(−x)
시그모이드 노드의 로컬 그래디언트는 다음과 같습니다.

∂y∂x=y(1−y)
계산그래프는 아래와 같습니다.
![Alt text](/images/2019-03-30-Mechine/ML_5.png)

출처링크 : https://ratsgo.github.io/deep%20learning/2017/05/14/backprop/
