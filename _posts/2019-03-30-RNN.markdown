---

layout: post

title: "[머신러닝] 기초정리(RNN)"

---
### RNN
RNN의 기본 구조
![Alt text](/images/2019-03-30-RNN/RNN_1.png)

RNN은 히든 노드가 방향을 가진 엣지로 연결돼 순환구조를 이루는(directed cycle) 인공신경망의 한 종류입니다. 음성, 문자 등 순차적으로 등장하는 데이터 처리에 적합한 모델로 알려져 있는데요. Convolutional Neural Networks(CNN)과 더불어 최근 들어 각광 받고 있는 알고리즘입니다.

위의 그림에서도 알 수 있듯 시퀀스 길이에 관계없이 인풋과 아웃풋을 받아들일 수 있는 네트워크 구조이기 때문에 필요에 따라 다양하고 유연하게 구조를 만들 수 있다는 점이 RNN의 가장 큰 장점입니다.

![Alt text](/images/2019-03-30-RNN/RNN_2.png)

RNN의 기본 구조는 위 그림과 같습니다. 녹색 박스는 히든 state를 의미합니다. 빨간 박스는 인풋 x, 파란 박스는 아웃풋 y입니다. 현재 상태의 히든 state ht는 직전 시점의 히든 state ht−1를 받아 갱신됩니다.

현재 상태의 아웃풋 yt는 ht를 전달받아 갱신되는 구조입니다. 수식에서도 알 수 있듯 히든 state의 활성함수(activation function)은 비선형 함수인 하이퍼볼릭탄젠트(tanh)입니다.

그런데 활성함수로 왜 비선형 함수를 쓰는걸까요? 밑바닥부터 시작하는 딥러닝의 글귀를 하나 인용해 보겠습니다.

선형 함수인 h(x)=cx를 활성 함수로 사용한 3층 네트워크를 떠올려 보세요. 이를 식으로 나타내면 y(x)=h(h(h(x)))가 됩니다. 이 계산은 y(x)=c∗c∗c∗x처럼 세번의 곱셈을 수행하지만 실은 y(x)=ax와 똑같은 식입니다. a=c3이라고만 하면 끝이죠. 즉 히든레이어가 없는 네트워크로 표현할 수 있습니다. 그래서 층을 쌓는 혜택을 얻고 싶다면 활성함수로는 반드시 비선형함수를 사용해야 합니다.

RNN의 기본 구조
RNN의 기본 동작을 직관적으로 이해해 보기 위해 CS231n 강좌의 Karpathy갓파시가 든 예제를 가져와 봤습니다. 어떤 글자가 주어졌을 때 바로 다음 글자를 예측하는 character-level-model을 만든다고 칩시다. 예컨대 RNN 모델에 ‘hell’을 넣으면 ‘o’를 반환하게 해 결과적으로는 ‘hello’를 출력하게 만들고 싶은 겁니다.

우선 우리가 가진 학습데이터의 글자는 ‘h’, ‘e’, ‘l’, ‘o’ 네 개뿐입니다. 이를 one-hot-vector로 바꾸면 각각 [1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]이 됩니다.


![Alt text](/images/2019-03-30-RNN/RNN_3.png)

x1은 [1,0,0,0]입니다. 이를 기반으로 h1인 [0.3,−0.1,0.9]를 만들었습니다(h0는 존재하지 않기 때문에 랜덤 값을 집어넣습니다). 이를 바탕으로 y1인 [1.0,2.2,−3.0,4.1]로 생성했습니다. 마찬가지로 두번째, 세번째, 네번째 단계들도 모두 갱신하게 됩니다. 이 과정을 순전파(foward propagation)라고 부릅니다.

다른 인공신경망과 마찬가지로 RNN도 정답을 필요로 합니다. 모델에 정답을 알려줘야 모델이 parameter를 적절히 갱신해 나가겠죠. 이 경우엔 바로 다음 글자가 정답이 되겠네요. 예컨대 ‘h’의 다음 정답은 ‘e’, ‘e’ 다음은 ‘l’, ‘l’ 다음은 ‘l’, ‘l’ 다음은 ‘o’가 정답입니다.

위의 그림을 기준으로 설명을 드리면 첫번째 정답인 ‘e’는 두번째 요소만 1이고 나머지가 0인 one-hot-vector입니다. 그림을 보면 아웃풋에 진한 녹색으로 표시된 숫자들이 있는데 정답에 해당하는 인덱스를 의미합니다. 이 정보를 바탕으로 역전파(backpropagation)를 수행해 parameter값들을 갱신해 나갑니다.

그렇다면 RNN이 학습하는 parameter는 무엇일까요? 인풋 x를 히든레이어 h로 보내는 Wxh, 이전 히든레이어 h에서 다음 히든레이어 h로 보내는 Whh, 히든레이어 h에서 아웃풋 y로 보내는 Why가 바로 parameter입니다. 그리고 모든 시점의 state에서 이 parameter는 동일하게 적용됩니다(shared weights).

RNN의 순전파
앞장에서 말씀드린 RNN의 기본 구조를 토대로 forward compute pass를 아래와 같이 그려봤습니다. 위에서 설명한 수식을 그래프로 옮겨놓은 것일 뿐입니다.

![Alt text](/images/2019-03-30-RNN/RNN_4.png)

RNN의 역전파
자, 이제 backward pass를 볼까요? 아래 그림과 같습니다. 혹시 역전파가 생소하신 분은 이곳을 참고하시기 바랍니다.

![Alt text](/images/2019-03-30-RNN/RNN_5.gif)

위 움짤과 아래 그림은 같은 내용입니다. 우선 forward pass를 따라 최종 출력되는 결과는 yt입니다. 최종 Loss에 대한 yt의 그래디언트(dL/dyt)가 RNN의 역전파 연산에서 가장 먼저 등장합니다. 이를 편의상 dyt라고 표기했고, 순전파 결과 yt와 대비해 붉은색으로 표시했습니다. 앞으로 이 표기를 따를 예정입니다.

dyt는 덧셈 그래프를 타고 양방향에 모두 그대로 분배가 됩니다. dWhy는 흘러들어온 그래디언트 dyt에 로컬 그래디언트 ht를 곱해 구합니다. dht는 흘러들어온 그래디언트 dyt에 Why를 곱한 값입니다. dhraw는 흘러들어온 그래디언트인 dht에 로컬 그래디언트인 1−tanh2(hraw)을 곱해 구합니다. 나머지도 동일한 방식으로 구합니다.

![Alt text](/images/2019-03-30-RNN/RNN_6.png)

다만 아래 그림에 주의할 필요가 있습니다. RNN은 히든 노드가 순환 구조를 띄는 신경망입니다. 즉 ht를 만들 때 ht−1가 반영됩니다. 바꿔 말하면 아래 그림의 dht−1은 t-1 시점의 Loss에서 흘러들어온 그래디언트인 Why∗dyt−1뿐 아니라 ★에 해당하는 그래디언트 또한 더해져 동시에 반영된다는 뜻입니다.

![Alt text](/images/2019-03-30-RNN/RNN_7.png)












출처
RNR
https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/